
<!-- saved from url=(0049)https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Graphical Models</title>
</head>

<body bgcolor="#FFFFFF">
<!-- white background is better for the pictures and equations -->


<h1>A Brief Introduction to Graphical Models and Bayesian Networks</h1>
By Kevin Murphy, 1998.

<!--, last updated 18 February 2003.<br>-->

<!--<img align=left src=Figures/thomas_bayes.gif>-->


<p>
<em>
"Graphical models are a marriage between probability theory and
graph theory. They provide a natural tool for dealing with two problems
that occur throughout applied mathematics and engineering --
uncertainty and complexity -- and in particular they are playing an
increasingly important role in the design and analysis of machine
learning algorithms. Fundamental to the idea of a graphical model is
the notion of modularity -- a complex system is built by combining
simpler parts. Probability theory provides the glue whereby the parts
are combined, ensuring that the system as a whole is consistent, and
providing ways to interface models to data. The graph theoretic side
of graphical models provides both an intuitively appealing interface
by which humans can model highly-interacting sets of variables as well
as a data structure that lends itself naturally to the design of
efficient general-purpose algorithms.
</em></p><p><em>
Many of the classical multivariate probabalistic systems studied in
fields such as statistics, systems engineering, information theory,
pattern recognition and statistical mechanics are special cases of the
general graphical model formalism -- examples include mixture models,
factor analysis, hidden Markov models, Kalman filters and Ising
models. The graphical model framework provides a way to view all of
these systems as instances of a common underlying formalism. This view
has many advantages -- in particular, specialized techniques that have
been developed in one field can be transferred between research
communities and exploited more widely. Moreover, the graphical model
formalism provides a natural framework for the design of new systems."
</em>
--- Michael Jordan, 1998.
</p><p>


</p><h2>This tutorial</h2>


We will briefly discuss the following topics.
<ul>

<li> <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#repr">Representation</a>, or, what exactly <em>is</em> a
graphical model?

</li><li> <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#infer">Inference</a>, or, how can we use these models
to efficiently answer probabilistic queries?

</li><li> <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#learn">Learning</a>, or, what do we do if we don't know what the
model is?

</li><li> <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#decision">Decision theory</a>, or, what happens when it
is time to convert beliefs into actions?

</li><li> <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#appl">Applications</a>, or, what's this all good for, anyway?
</li></ul>


<p>
<!--
Note: <a href="http://www.ece.ogi.edu/~strom/">Dan Hammerstrom</a>
has made a <a href="bayes_tutorial.pdf">pdf</a> version of this web
page.
-->
Note: (a version of) this page is available
<b>in pdf format</b>
<a href="https://www.cs.ubc.ca/~murphyk/Papers/intro_gm.pdf">here</a>.
Also, 
Marie Stefanova has made
a Swedish translation
<a href="https://www.bildeleekspert.dk/blog/2017/06/28/krotkie-wprowadzenie-modeli-graficznych-sieci-bayesa/">here</a>.

</p><h2>Articles in the popular press</h2>

The following articles provide less technical introductions.

<ul>
<!--
<li><a
href="http://news.com.com/2009-1001-984695.html?tag=fd_lede1_hed">CNET</a>
article by Michael Kanellos (2/18/03). 
-->
<li><a href="https://www.cs.ubc.ca/~murphyk/Bayes/la.times.html">LA times</a> article (10/28/96)
about Bayes nets.
</li><li> <a href="https://www.cs.ubc.ca/~murphyk/Bayes/econ.22mar01.html">Economist article</a> (3/22/01)
about Microsoft's application of BNs.
</li></ul>


<h2>Other sources of technical information</h2>

<ul>
<li> <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bayesrule.html">My tutorial on Bayes rule</a>
</li><li> <a href="http://www.auai.org/">AUAI homepage </a>
(Association for Uncertainty in Artificial Intelligence)
</li><li> <a href="http://www.cs.orst.edu/~dambrosi/uai.html">The UAI
mailing list</a>
</li><li> <a href="http://rome.exp.sis.pitt.edu/UAI/uai.asp">UAI proceedings</a>.
</li><li> My list of <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#reading">recommended reading</a> .
</li><li> <a href="https://www.cs.ubc.ca/~murphyk/Software/bnsoft.html">Bayes Net software
packages</a>
</li><li> My <a href="https://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.html">Bayes Net Toolbox</a> for
<a href="http://www.mathworks.com/">Matlab</a>
</li><li> <a href="https://www.cs.ubc.ca/~murphyk/Software/BNT/Talks/BNT_mathworks.ppt">Tutorial slides on graphical models
and BNT</a>, presented to the Mathworks, May 2003
</li><li> <a href="http://www.cs.engr.uky.edu/~dekhtyar/dblab/resources.html#bnets">List
of other Bayes net tutorials</a>
</li></ul>
  





<h1><a name="repr">Representation</a></h1><a name="repr">

Probabilistic graphical models are graphs in which nodes represent
random variables, and the (lack of) arcs represent conditional
independence assumptions.
Hence they provide a compact representation of joint probability
distributions.
<em>Undirected</em> graphical models, also called Markov
Random Fields (MRFs) or Markov networks,
have a simple definition of independence: two (sets 
of) nodes A and B are conditionally independent given a third set, C,
if all paths between the nodes in A and B are separated by a node in C.
By contrast, <em>directed</em> graphical models
also called Bayesian 
Networks or Belief Networks  (BNs), have a more
complicated notion of independence,
which takes into account the directionality of the
arcs, as we explain below.

<!--
<p>
In the examples below, we will mostly focus on examples where the
nodes are <b>discrete</b> random variables, since this has been the
focus of most of the work in the AI community.
Discrete variables are usually simpler to deal with,
both mathematically and computationally,
than continuous random variables.
However, many random variables (eg unknown parameters) are naturally
continuous.
A <b>hierarchical Bayesian model</b> is just a Bayes net
where some of the nodes represent unknown parameters
(see eg.
the <a href="http://www.mrc-bsu.cam.ac.uk/bugs/welcome.shtml">BUGS</a>
system).
In the AI community, it is more common
to construct the parameters by hand (eg from an expert), or to use
frequentist (maximum likelihood) learning techniques to estimate them.
We will adopt this convention for simplicity below.
(But see
<a href="bayesrule#reading">some readings on Bayesian statistics</a>.)
-->
<p>
Undirected graphical models are more popular with the physics and
vision communities, and directed models are more popular with the AI
and statistics communities. (It is possible to have a model with both
directed and undirected arcs, which is called a chain graph.)
For a careful study of the relationship between directed and
undirected graphical models, see the books by Pearl88, Whittaker90,
and Lauritzen96.

</p></a><p><a name="repr">

Although directed models have a more complicated notion of
independence than undirected models,
they do have several advantages.
The most important is that
one can regard an arc from A to B as
indicating that A ``causes'' B. 
(See the discussion on </a><a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#causality">causality</a>.)
This can be used as a guide to construct the graph structure.
In addition, directed models can encode deterministic
relationships, and are easier to learn (fit to data).
In the rest of this tutorial, we will only discuss directed graphical
models, i.e., Bayesian networks.


</p><p>
In addition to the graph structure, it is necessary to specify the
parameters of the model.
<!--
For an undirected model, we must specify a "potential function" for
each clique in the graph.
-->
For a directed model, we must specify
the Conditional Probability Distribution (CPD) at each node.
If the variables are discrete, this can be represented as a table
(CPT), which lists the probability that the child node takes on each
of its different values for each combination of values of its
parents. Consider the following example, in which all nodes are binary,
i.e., have two possible values, which we will denote by T (true) and
F (false).


</p><p>
</p><center>
<img src="./Graphical Models_files/sprinkler.gif">
</center>
<p>

We see that the event "grass is wet" (W=true) has two 
possible causes: either the water sprinker is on (S=true) or it is
raining (R=true).
The strength of this relationship is shown in the table.
For example, we see that Pr(W=true | S=true, R=false) = 0.9 (second
row), and
hence, Pr(W=false | S=true, R=false) = 1 - 0.9 = 0.1, since each row
must sum to one.
Since the C node has no parents, its CPT specifies the prior
probability that it is cloudy (in this case, 0.5).
(Think of C as representing the season:
if it is a cloudy season, it is less likely that the sprinkler is on
and more likely that the rain is on.)
</p><p>
The simplest conditional independence relationship encoded in a Bayesian
network can be stated as follows:
a node is independent of its ancestors given its parents, where the
ancestor/parent relationship is with respect to some fixed topological
ordering of the nodes.
</p><p>
By the chain rule of probability,
the joint probability of all the nodes in the graph above is
</p><pre>P(C, S, R, W) = P(C) * P(S|C) * P(R|C,S) * P(W|C,S,R)
</pre>
By using conditional independence relationships, we can rewrite this as
<pre>P(C, S, R, W) = P(C) * P(S|C) * P(R|C)   * P(W|S,R)
</pre>
where we were allowed to simplify the third term because R is
independent of S given its parent C, and the last term because W is
independent of C given its parents S and R.
<!--
<IMG ALIGN=BOTTOM SRC="bformulas/_10934_displaymath20.gif">
<IMG ALIGN=BOTTOM SRC="Figures/eqn_PCSRW.gif">
-->
<p>
We can see that the conditional independence relationships
allow us to <em>represent</em> the joint more compactly.
Here the savings are minimal, but in general, if we had n binary
nodes, the full joint would require O(2^n) space to represent, but the
factored form would require O(n 2^k) space to represent, where k is
the maximum fan-in of a node. And fewer parameters makes learning easier.

</p><h3>Are "Bayesian networks" Bayesian?</h3>

Despite the name,
Bayesian networks do not necessarily imply a commitment to Bayesian
statistics. 
Indeed, it is common to use 
frequentists methods to estimate the parameters of the CPDs.
Rather, they are so called because they use
Bayes' rule for
probabilistic inference, as we explain below.
(The term "directed graphical model" is perhaps more appropriate.)
Nevetherless, Bayes nets are a useful representation for hierarchical
Bayesian models, which form the foundation of applied Bayesian
statistics
(see e.g., the <a href="http://www.mrc-bsu.cam.ac.uk/bugs/welcome.shtml">BUGS</a> project).
In such a model, the parameters are treated like any other random
variable, and becomes nodes in the graph.



<h2>Inference</h2>

The most common task we wish to solve using Bayesian networks is
probabilistic inference. For example, consider the water sprinkler
network, and suppose we observe the
fact that the grass is wet. There are two possible causes for this:
either it is raining, or the sprinkler is on. Which is more likely?
We can use <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bayesrule.html">Bayes' rule</a> to compute the posterior probability of each
explanation (where 0==false and 1==true).
<!--
<pre>
Pr(S=t | W=t) = Pr(S=t, W=t) / Pr(W=t)
 = sum_{c, r} Pr(C=c,S=t,R=r,W=t)  / Pr(W=t) = 0.2781 / 0.6471 = 0.4298
<br>
Pr(R=t | W=t) = Pr(S=t, W=t) / Pr(W=t)
 = sum_{c, r} Pr(C=c,S=t,R=r,W=t)  / Pr(W=t) = 0.4591 / 0.6471 = 0.7079
</pre>
-->
<p><img align="BOTTOM" src="./Graphical Models_files/_10934_displaymath21.gif">
</p><p><img align="BOTTOM" src="./Graphical Models_files/_10934_displaymath22.gif">
<!--
<P><IMG ALIGN=BOTTOM SRC="Figures/eqn_Pr_SgivenW.gif"><P>
<P><IMG ALIGN=BOTTOM SRC="Figures/eqn_Pr_RgivenW.gif"><P>
-->
<br>
where
<br>
<!--
<pre>
Pr(W=t) = sum_{c, r, s} Pr(C=c,S=s,R=r,W=t) = 0.6471
</pre>
-->
</p><p><img align="BOTTOM" src="./Graphical Models_files/_10934_displaymath23.gif">
<!--
<P><IMG ALIGN=BOTTOM SRC="Figures/eqn_PW.gif">
-->
<br>
is a normalizing constant, equal to the probability (likelihood) of
the data.
So we see that it is more likely that the grass is wet because
it is raining:
the likelihood ratio is 0.7079/0.4298 = 1.647.


</p><h2><a name="explainaway">Explaining away</a></h2><a name="explainaway">

In the above example, notice that the two causes "compete" to "explain" the
observed data. Hence S and R become conditionally dependent given that
their common child, W, is observed, even though they are marginally
independent. For example,
suppose the grass is wet, but that we also know that it is raining.
Then the posterior probability that the sprinkler is on goes down:
<pre>Pr(S=1|W=1,R=1) = 0.1945
</pre>
This is called "explaining away".
In statistics, this is known as Berkson's paradox, or "selection
bias". For a dramatic example of this effect, consider a college which
admits students who are either brainy or sporty (or both!).
Let C denote the event that someone is admitted to college, which is
made true if they are either brainy (B) or sporty (S).
Suppose in the general population, B and S are independent.
We can model our conditional independence assumptions using a graph
which is a V structure, with arrows pointing down: 
<pre>   B   S
    \ /
     v
     C
</pre>
Now look at a population of college students (those for which C is
observed to be true).
It will be found that being brainy makes you less likely to be sporty
and vice versa, because either property alone is sufficient to explain
the evidence on C
(i.e., P(S=1 | C=1, B=1) &lt;= P(S=1 | C=1)).
(If you don't believe me,
</a><a href="https://www.cs.ubc.ca/~murphyk/Bayes/brainy.m">try this little BNT demo</a>!)

<h2>Top-down and bottom-up reasoning</h2>

In the water sprinkler example, we had evidence of an effect (wet grass), and
inferred the most likely cause. This is called diagnostic, or "bottom
up", reasoning, since it goes
from effects to causes; it is a common task in expert systems.
Bayes nets can also be used for causal, or "top down",
reasoning. For example, we can compute the probability that the grass
will be wet given that it is cloudy.
Hence Bayes nets are often called "generative" models, because they
specify how causes generate effects.
<!--
<p>
The above example showed what Bayes nets can be used for.
Unfortunately, the method of first computing the full joint
probability distribution, and then marginalizing
out the unwanted nodes, takes time which is exponential in the number
of nodes.
As we will see below, the key to efficient inference is to exploit the
factored form of the joint, i.e., the conditional independence
properties of the model.
-->



<h2><a name="causality">Causality</a></h2><a name="causality">

One of the most exciting things about Bayes nets is that they can be
used to put discussions about causality on a solid mathematical basis.
One very interesting question is: can we distinguish causation from mere
correlation? The answer is "sometimes",
but you need to measure the relationships between <em>at least
three</em> variables; the intution is that one of the variables acts
as a "virtual control" for the relationship between the other two,
so we don't always need to do experiments to infer causality.
See the following books for details.

</a><ul><a name="causality">
</a><li><a name="causality"> </a><a href="http://bayes.cs.ucla.edu/BOOK-2K/index.html">"Causality: Models,
Reasoning and Inference"</a>,
Judea Pearl, 2000, Cambridge University Press. 

</li><li> <a href="http://hss.cmu.edu/html/departments/philosophy/TETRAD/tetrad.html">"Causation,
Prediction and Search"</a>, Spirtes, Glymour and 
Scheines, 2001 (2nd edition), MIT Press.

</li><li> <a href="http://callisto.si.usherb.ca:8080/bshipley/my%20book.htm">"Cause
and Correlation in Biology"</a>, Bill Shipley, 2000,
Cambridge University Press.

</li><li> "Computation, Causation and Discovery", Glymour and Cooper (eds),
1999, MIT Press.
</li></ul>



<h2>Conditional independence in Bayes Nets</h2>

In general,
the conditional independence relationships encoded by a Bayes Net
are best be explained by means of the "Bayes Ball"
algorithm (due to Ross Shachter), which is as follows:
Two (sets of) nodes A and B are conditionally independent
(d-separated) given a set C
if and only if there is no
way for a ball to get from A to B in the graph, where the allowable
movements of the ball are shown below.
Hidden nodes are nodes whose values
are not known, and are depicted as unshaded; observed nodes (the ones
we condition on) are shaded.
The dotted arcs indicate direction of flow of the ball.


<p>
</p><center>
<img src="./Graphical Models_files/bayes_ball_no_det.gif">
</center>
<p>

The most interesting case is the first column, when we have two arrows converging on a
node X (so X is a "leaf" with two parents).
If X is hidden, its parents are marginally independent, and hence the
ball does not pass through (the ball being "turned around" is
indicated by the curved arrows); but if X is observed, the parents become
dependent, and the ball does pass through,
because of the explaining away phenomenon.
<!--
This phenomenon is called "explaining away", since the two parents
"compete" to "explain" the observation on X, and hence become correlated (see the
example below).
-->
Notice that, if this graph was undirected, the child
would always separate the parents; hence when converting a directed
graph to an undirected graph, we must add links between "unmarried"
parents who share a common child (i.e., "moralize" the graph) to prevent us reading off incorrect
independence statements.

</p><p>
Now consider the second column in which we have two diverging arrows from X (so X is a
"root").
If X is hidden,
the children are dependent, because they have a hidden common cause,
so the ball passes through.
If X is observed, its children are rendered conditionally
independent, so the ball does not pass through.
Finally, consider the case in which we have one incoming and outgoing
arrow to X. It is intuitive that the nodes upstream
and downstream of X are dependent iff X is hidden, because
conditioning on a node breaks the graph at that point.
<!--
When we have deterministic nodes, we need to modify the Bayes ball
algorithm slightly. Click <a href="deterministic.html">here</a> for details.
-->






</p><h2>Bayes nets with discrete and continuous nodes</h2>

The introductory example used nodes with categorical values and multinomial distributions.
It is also possible to create Bayesian networks with continuous valued nodes.
The most common distribution for such variables is the Gaussian.
For discrete nodes with continuous parents, we can use the
logistic/softmax distribution.
Using multinomials, conditional Gaussians, and the softmax
distribution, we can have a rich toolbox for making complex models.
Some examples are shown below. For details, click
<a href="http://bnt.googlecode.com/svn/trunk/docs/usage.html#examples">here</a>.
 
(Circles denote continuous-valued random variables,
squares denote discrete rv's, clear
means hidden, and shaded means observed.)

<p>
</p><center>
<table>
<tbody><tr>
<td><img src="./Graphical Models_files/mixexp.gif">
</td><td><img src="./Graphical Models_files/hme.gif">
</td></tr></tbody></table>
</center>

<center>
<table>
<tbody><tr>
<td><img src="./Graphical Models_files/fa.gif">
</td><td><img src="./Graphical Models_files/mfa.gif">
</td><td><img src="./Graphical Models_files/fa_scalar.gif">
</td><td><img src="./Graphical Models_files/ifa.gif">
</td></tr></tbody></table>
</center>

For more details, see this excellent paper.
<ul>
<li> <a href="http://www.cs.toronto.edu/~roweis/papers/NC110201.pdf">
A Unifying Review of Linear Gaussian Models</a>,
Sam Roweis &amp; Zoubin Ghahramani.
Neural Computation 11(2) (1999) pp.305-345
</li></ul>


<h2><a name="dbn">Temporal models</a></h2><a name="dbn">



Dynamic Bayesian Networks (DBNs) are directed graphical models of stochastic
processes.
They generalise </a><a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#hmm">hidden Markov models (HMMs)</a>
and <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#lds">linear dynamical systems (LDSs)</a>
by representing the hidden (and observed) state in terms of state
variables, which can have complex interdependencies.
The graphical structure provides an easy way to specify these
conditional independencies, and hence to provide a compact
parameterization of the model.
<p>
Note that "temporal Bayesian network" would be a better name than
"dynamic Bayesian network", since
it is assumed that the model structure does not change, but
the term DBN has become entrenched.
We also normally assume that the parameters do not
change, i.e., the model is time-invariant.
However, we can always add extra
hidden nodes to represent the current "regime", thereby creating
mixtures of models to capture periodic non-stationarities.
There are some cases where the size of the state space can change over
time, e.g., tracking a variable, but unknown, number of objects.
In this case, we need to change the model structure over time.
</p><p>


</p><h3><a name="hmm">Hidden Markov Models (HMMs)</a></h3><a name="hmm">

The simplest kind of DBN is a Hidden Markov Model (HMM), which has
one discrete hidden node and one discrete or continuous
observed node per slice. We illustrate this below.
As before, circles denote continuous nodes, squares denote
discrete nodes, clear means hidden, shaded means observed.
<!--
(The observed nodes can be
discrete or continuous; the crucial thing about an HMM is that the
hidden nodes are discrete, so the system can model arbitrary dynamics
-- providing, of course, that the hidden state space is large enough.)
-->
<p>
<img src="./Graphical Models_files/hmm4.gif">
</p><p>
We have "unrolled" the model for 4 "time slices" -- the structure and parameters are
assumed to repeat as the model is unrolled further.
Hence to specify a DBN, we need to
define the intra-slice topology (within a slice),
the inter-slice topology (between two slices),
as well as the parameters for the first two slices.
(Such a two-slice temporal Bayes net is often called a 2TBN.)
</p><p>
Some common variants on HMMs are shown below.
</p><p>
</p><center>
<img src="./Graphical Models_files/hmm_zoo.gif">
</center>
<p>
<!--
<p>
<center>
<table>
<tr>
<td><img src="Figures/hmm_gauss.gif">
<td><img src="Figures/hmm_mixgauss.gif"
<td><img src="Figures/hmm_ar.gif">
<tr>
<td><img src="Figures/hmm_factorial.gif">
<td><img src="Figures/hmm_coupled.gif"
<td><img src="Figures/hmm_io.gif">
<tr>
</table>
</center>
-->

</p></a><h3><a name="hmm"></a><a name="lds">Linear Dynamical Systems (LDSs) and Kalman filters</a></h3><a name="lds">

A Linear Dynamical System (LDS) has the same topology as an HMM, but
all the nodes are assumed to have linear-Gaussian distributions, i.e.,
<pre>   x(t+1) = A*x(t) + w(t),  w ~ N(0, Q),  x(0) ~ N(init_x, init_V)
   y(t)   = C*x(t) + v(t),  v ~ N(0, R)
</pre>
The Kalman filter is a way of doing online filtering in this model.
Some simple variants of LDSs are shown below.
<p>
</p><center>
<table>
<tbody><tr>
<td><img src="./Graphical Models_files/ar1.gif">
</td><td><img src="./Graphical Models_files/sar.gif">
</td><td><img src="./Graphical Models_files/kf.gif">
</td><td><img src="./Graphical Models_files/skf.gif">
</td></tr></tbody></table>
</center>
<p>

The Kalman filter has been proposed as a model for how the brain
integrates visual cues over time to infer the state of the world,
although the reality is obviously much more complicated.
The main point is not that the Kalman filter is the right model, but that
the brain is combining bottom up and top down cues.
The figure below is from a paper called
"A Kalman Filter Model of the Visual Cortex",
by P. Rao, Neural Computation 9(4):721--763, 1997.

</p><center>
<img src="./Graphical Models_files/kfhead.jpg">
</center>

<h3>More complex DBNs</h3>

It is also possible to create temporal models with much more complicated
topologies, such as the Bayesian Automated Taxi (BAT) network shown
below.
(For simplicity, we only show the observed leaves for slice 2.
Thanks to Daphne Koller for providing this figure.)
<p>
<img src="./Graphical Models_files/batnet.gif">
</p></a><p><a name="lds">
When some of the observed nodes are thought of as inputs (actions), and some as
outputs (percepts), the DBN becomes a </a><a href="https://www.cs.ubc.ca/~murphyk/Bayes/pomdp.html">POMDP</a>.
See also the section on <a href="https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html#decision">decision theory</a> below.


</p><h2>A generative model for generative models</h2>

<!--
For a good summary of how various popular Bayes nets inter-relate
click <a href="Figures/gmka.gif">here</a>.
(This was produced by Zoubin Ghahramani and Sam Roweis.)
-->
The figure below, produced by Zoubin Ghahramani and Sam Roweis, is a
good summary of the relationships between some popular graphical models.
<p>

<img src="./Graphical Models_files/gmka.gif">




</p><h1><a name="infer">INFERENCE</a></h1><a name="infer">

A graphical model specifies a complete
joint probability distribution (JPD) over all the variables.
Given the JPD, we can answer all possible inference
queries by marginalization (summing out over irrelevant variables), as
illustrated in the introduction. However, the JPD has size O(2^n), where n is the
number of nodes, and we have assumed each node can have 2
states. Hence summing over the JPD takes exponential time.
We now discuss more efficient methods.

<h2>Variable elimination</h2>

For a directed graphical model (Bayes net), 
we can sometimes use the factored representation of the JPD
to do marginalisation efficiently.
The key idea is to "push sums in" as far as possible when summing
(marginalizing) out irrelevant terms, e.g., for the water sprinkler network

<!--
<p>
Pr(W=w) = sum_c sum_s sum_r   Pr(C=c) * Pr(S=s|C=c) * Pr(R=r|C=c) * Pr(W=w|S=s,R=r)
<br>
Pr(W=w) = sum_c Pr(C=c) * sum_s Pr(S=s|C=c) * sum_r Pr(R=r|C=c) * Pr(W=w|S=s,R=r)
<p>
-->
<p><img align="BOTTOM" src="./Graphical Models_files/_10934_eqnarray12.gif"></p><p>

Notice that, as we perform the innermost sums, we create new terms,
which need to be summed over in turn e.g.,

</p><p>
<!--Pr(W=w) = sum_c Pr(C=c) * sum_s Pr(S=s|C=c) * T1(c,w,s)-->
</p><p><img align="BOTTOM" src="./Graphical Models_files/_10934_displaymath24.gif"></p><p>
where
<!--T1(c,w,s) = sum_r Pr(R=r|C=c) * Pr(W=w|S=s,R=r)-->
</p><p><img align="BOTTOM" src="./Graphical Models_files/_10934_displaymath25.gif"></p><p>
Continuing in this way,
</p><p><img align="BOTTOM" src="./Graphical Models_files/_10934_displaymath26.gif"></p><p>
where
</p><p><img align="BOTTOM" src="./Graphical Models_files/_10934_displaymath27.gif"></p><p>

</p><p>
This algorithm is called Variable Elimination.
The principle of distributing sums over products can be generalized
greatly to apply to any commutative semiring.
This forms the basis of many common algorithms, such as Viterbi
decoding and the Fast Fourier Transform. For details, see

</p></a><ul><a name="infer">

</a><li><a name="infer"> R. McEliece and S. M. Aji, 2000.
<!--<a href="http://www.systems.caltech.edu/EE/Faculty/rjm/papers/GDL.ps">-->
</a><a href="https://www.cs.ubc.ca/~murphyk/Bayes/GDL.pdf">
The Generalized Distributive Law</a>,
IEEE Trans. Inform. Theory, vol. 46, no. 2 (March 2000),
pp. 325--343. 

</li><li>
F. R. Kschischang, B. J. Frey and H.-A. Loeliger, 2001.
<a href="http://www.cs.toronto.edu/~frey/papers/fgspa.abs.html">
Factor graphs and the sum-product algorithm</a>
IEEE Transactions on Information Theory, February, 2001.

</li></ul>

<p>

The amount of work we perform when computing a marginal is bounded by
the size of the largest term that we encounter. Choosing a summation
(elimination) ordering to
minimize this is NP-hard, although greedy algorithms work well in
practice.



</p><h2>Dynamic programming</h2>

If we wish to compute several marginals at the same time, we can use Dynamic
Programming (DP) to avoid the redundant computation that would be involved
if we used variable elimination repeatedly.
If the underlying undirected graph of the BN is acyclic (i.e., a tree), we can use a
local message passing algorithm due to Pearl.
<!--(Click <a href="pearl.ps">here</a> to download a short derivation
of Pearl's algorithm.)-->
This is a generalization of the well-known forwards-backwards
algorithm for HMMs (chains).
For details, see
<ul>
<li> "Probabilistic Reasoning in Intelligent Systems", Judea Pearl,
1988, 2nd ed.
</li><li> "Fusion and propogation with multiple observations in belief networks",
 Peot and Shachter, AI 48 (1991) p. 299-318.
</li></ul>


<p>
If the BN has undirected cycles (as in the water sprinkler example), 
local message passing algorithms run the risk of double counting.
e.g., the information from S and R flowing
into W is not independent, because it came from a common cause, C.
The most common approach is therefore to convert the BN into a tree,
by clustering nodes together, to form what is called a
junction tree, and then running a local message passing algorithm on
this tree. The message passing scheme could be Pearl's algorithm, but
it is more common to use a variant designed for undirected models.
For more details, click <a href="https://www.cs.ubc.ca/~murphyk/Bayes/jtree.html">here</a>

</p><p>
The running time of the DP algorithms is exponential in the size of
the largest cluster (these clusters correspond to the intermediate
terms created by variable elimination). This size is called the
induced width of the graph. Minimizing this is NP-hard.


</p><h2>Approximation algorithms</h2>

Many models of interest,
such as those with repetitive structure, as in
multivariate time-series or image analysis,
have large induced width, which makes exact
inference very slow.
<!--
(Exact inference is NP-hard in the worst case.)
-->
We must therefore resort to approximation techniques.
Unfortunately, approximate inference is #P-hard, but we can nonetheless come up
with approximations which often work well in practice. Below is a list
of the major techniques.

<ul>

<li> Variational methods.
The simplest example is the mean-field approximation,
which exploits the law of
large numbers to approximate large sums of random variables by their
means. In particular, we essentially decouple all the nodes, and
introduce a new parameter, called a variational parameter, for each
node, and iteratively update these parameters so as to minimize the
cross-entropy (KL distance) between the approximate and true
probability distributions. Updating the variational parameters becomes a proxy for
inference. The mean-field approximation produces a lower bound on the
likelihood. More sophisticated methods are possible, which give
tighter lower (and upper) bounds.
<p>

</p></li><li> Sampling (Monte Carlo) methods. The simplest kind is importance
sampling, where we draw random samples x from P(X), the (unconditional)
distribution on the hidden variables, and
then weight the samples by their likelihood, P(y|x), where y is the
evidence. A more efficient approach in high dimensions is called Monte
Carlo Markov Chain (MCMC), and 
includes as special cases Gibbs sampling and the Metropolis-Hasting algorithm.
<p>

</p></li><li> "Loopy belief propogation". This  entails
applying Pearl's algorithm to the original
graph, even if it has loops (undirected cycles).
In theory, this runs the risk of double counting, but Yair Weiss and
others have proved that in certain cases (e.g., a single loop), events are double counted
"equally", and hence "cancel" to give the right answer.
Belief propagation is equivalent to exact inference on a modified
graph, called the universal cover or unwrapped/ computation tree,
which has the same local topology as the original graph.
This is the same as the Bethe and cavity/TAP approaches in statistical
physics.
Hence there is a deep connection between
belief propagation and variational methods that people are currently investigating.
<p>

</p></li><li> Bounded cutset conditioning. By instantiating subsets of the variables,
we can break loops in the graph.
Unfortunately, when the cutset is large, this is very slow.
By instantiating only a subset of values of the cutset, we can compute
lower bounds on the probabilities of interest.
Alternatively, we can sample the cutsets jointly, a technique known as block Gibbs sampling.
<p>

</p></li><li> Parametric approximation methods.
These express the intermediate summands in a simpler
form, e.g., by approximating them as a product of smaller factors.
"Minibuckets" and the Boyen-Koller algorithm fall into this category.
<p>

</p></li></ul>
Approximate inference is a huge topic:
see the references for more details.




<h2>Inference in DBNs</h2>

The general inference problem for DBNs is to compute
P(X(i,t0) | y(:, t1:t2)), where X(i,t) represents the i'th hidden
variable at time and t Y(:,t1:t2) represents all the evidence
between times t1 and t2. 
(In fact, we often also want to compute joint distributions of
variables over one or more time slcies.)
There are several special cases of interest, illustrated below.
The arrow indicates t0: it is X(t0) that we are trying to estimate.
The shaded region denotes t1:t2, the available data.
<p>

<img src="./Graphical Models_files/filter.gif">

<!--
The general inference problem for DBNs is to compute
P(X(i,t) | Y(:, t1:t2)), where X(i,t) represents the i'th hidden
variable at time and t Y(:,t1:t2) represents all the evidence
between times t1 and t2. 
(Of course, we might want to compute joint distributions over sets of
hidden variables, especially if we want to do learning.)
There are several special cases of interest:
<ul>
<li> Online filtering: P(X(i,t) | Y(:, 1:t))
<li> Offline fixed interval smoothing: P(X(i,t) | Y(:, 1:T)), where T is the length
of the sequence
<li> Online fixed lag smoothing: P(X(i,t) | Y(:, t-lag:t))
<li> Online prediction: P(X(i,t+lead) | Y(:, 1:t))
</ul>
-->

</p><p>
Here is a simple example of inference in an LDS.
Consider a particle moving in the plane at
constant velocity subject to random perturbations in its trajectory.
The new position (x1, x2) is the old position plus the velocity (dx1,
dx2) plus noise w.
</p><pre>[ x1(t)  ] =  [1 0 1 0] [ x1(t-1)  ] + [ wx1  ]
[ x2(t)  ]    [0 1 0 1] [ x2(t-1)  ]   [ wx2  ]
[ dx1(t) ]    [0 0 1 0] [ dx1(t-1) ]   [ wdx1 ]
[ dx2(t) ]    [0 0 0 1] [ dx2(t-1) ]   [ wdx2 ]
</pre>
We assume we only observe the position of the particle.
<pre>[ y1(t) ] =  [1 0 0 0] [ x1(t)  ] + [ vx1 ]
[ y2(t) ]    [0 1 0 0] [ x2(t)  ]   [ vx2 ]
                       [ dx1(t) ] 
                       [ dx2(t) ]
</pre>
Suppose we start out at position (10,10) moving to the right with
velocity (1,0).
We sampled a random trajectory of length 15.
Below we show the filtered and smoothed trajectories.

<table>
<tbody><tr><td>
<img src="./Graphical Models_files/aima_filtered.jpg">
</td><td>
<img src="./Graphical Models_files/aima_smoothed.jpg">
</td></tr></tbody></table>

The mean squared error of the filtered estimate is 4.9; for the
smoothed estimate it is 3.2.
Not only is the smoothed estimate better, but we know that
it is better, as illustrated by the smaller uncertainty ellipses;
this can help in e.g., data association problems.
Note how the smoothed ellipses are larger at the ends, because these
points have seen less data. Also, note how rapidly the filtered
ellipses reach their steady-state (Ricatti) values.
(See my <a href="http://www.ai.mit.edu/~murphyk/Software/Kalman/kalman.html">Kalman
filter toolbox</a> for more details.)




<h1><a name="learn">LEARNING</a></h1><a name="learn">

One needs to specify two things to describe a BN: the graph topology
(structure) and the parameters of each CPD. It is possible to learn
both of these from data. However, learning structure is much harder
than learning parameters. Also, learning when some of the nodes are
hidden, or we have missing data, is much harder than when everything
is observed. This gives rise to 4 cases:

<pre>Structure   Observability    Method
---------------------------------------------
Known       Full             Maximum Likelihood Estimation
Known       Partial          EM (or gradient ascent)
Unknown     Full             Search through model space 
Unknown     Partial          EM + search through model space 
</pre>


<!-- for more details, click <a href="learn.html">here</a>.-->

</a><h2><a name="learn"></a><a name="sec:known_struct_full_obs">Known structure, full observability</a></h2><a name="sec:known_struct_full_obs">


We assume that the goal of learning in this case is to find the values
of the parameters of each CPD which maximizes the likelihood of the
training data,
which contains N cases (assumed to be independent).
The normalized log-likelihood of the training set D
<!--L = \frac{1}{N} \log \Pr(D|G), where D = \{D_1, \ldots, D_S\},-->
is a sum of terms, one
for each node:
<!-- L = \frac{1}{N} \sum_{i=1}^{m} \sum_{l=1}^S \log P(X_i | Pa(X_i), D_l)-->
<center>
<img src="./Graphical Models_files/loglik.gif">
</center>

We see that the log-likelihood scoring function <em>decomposes</em>
according to the structure of the graph, and hence we can maximize the
contribution to the log-likelihood of each node independently
(assuming the parameters in each node are independent of the other nodes).
<!--
<p>
In cases where N is small compared to the number of parameters that
require fitting, we can use a numerical prior to regularize the
problem. In this case, we call the estimates Maximum A
Posterori (MAP) estimates, as opposed to Maximum Likelihood (ML) estimates.
-->
<p>
Consider estimating the Conditional Probability Table for the W
node. If we have a set of training data, we can just count the number
of times the grass is wet when it is raining and the sprinler is on,
N(W=1,S=1,R=1), the number of times the grass is wet when it is
raining and the sprinkler is off, N(W=1,S=0,R=1), etc. Given these
counts (which are the sufficient statistics), we can find the Maximum
Likelihood Estimate of the CPT as follows:
</p><p>
<img align="BOTTOM" src="./Graphical Models_files/_10934_displaymath28.gif">
</p><p>
where the denominator is N(S=s,R=r) = N(W=0,S=s,R=r) + N(W=1,S=s,R=r).
Thus "learning" just amounts to counting (in the case of multinomial
distributions).
For Gaussian nodes, we can compute the sample mean and variance, and
use linear regression to estimate the weight matrix.
For other kinds of distributions, more complex procedures are
necessary.
</p><p>
As is well known from the HMM literature, ML estimates of CPTs are
prone to sparse data problems, which can be solved by using (mixtures
of) Dirichlet priors (pseudo counts).
This results in a Maximum A Posteriori (MAP) estimate.
For Gaussians, we can use a Wishart prior, etc.

<!--
<h3><a name="bayesian">Bayesian learning</h3>

In principle, it is straightforward to use
graphical models to do
Bayesian learning: the parameters, being random variables, become
nodes as well, and the goal is the standard inference problem of
computing posterior distributions on the (parameter) nodes. In
practice, however, it is usually impossible to do this analytically,
so we must approximate inference techniques, typically MCMC.
-->


</p></a><h2><a name="sec:known_struct_full_obs"></a><a name="sec:known_struct_partial_obs">Known structure, partial observability</a></h2><a name="sec:known_struct_partial_obs">

When some of the nodes are hidden, we can use the EM (Expectation
Maximization) algorithm to find a (locally) optimal Maximum Likelihood
Estimate of the parameters.
The basic idea behind EM is that, if we knew the values of
all the nodes, learning (the M step) would be easy, as we saw
above. So in the E step, we compute the expected values of all the
nodes using an inference algorithm, and then treat these expected
values as though they were observed (distributions). For example, in the case
of the W node, we replace the observed counts of the events with the number of times
we expect to see each event:
<pre>P(W=w|S=s,R=r) = E N(W=w,S=s,R=r) / E N(S=s,R=r)
</pre>
<p>
where
E N(x) is the expected number of times event x
occurs in the whole training set, given the current guess of the parameters.
These expected counts can be computed as follows
</p><p>
</p><pre>E N(.) = E sum_k I(. | D(k)) = sum_k P(. | D(k))
</pre>
<p>
where I(x | D(k)) is an indicator function which is 1 if event
x occurs in training case k, and 0 otherwise.
</p><p>
Given the expected counts, we maximize the parameters, and then
recompute the expected counts, etc. This iterative procedure is
guaranteed to converge to a local maximum of the likelihood surface.
It is also possible to do gradient ascent on the likelihood surface (the
gradient expression also involves the expected counts), but EM is
usually faster (since it uses the natural gradient) and simpler (since
it has no step size parameter and
takes care of parameter constraints (e.g., the "rows" of the
CPT having to sum to one) automatically).
In any case, we see than when nodes are hidden, inference becomes a
subroutine which is called by the learning procedure; hence fast
inference algorithms are crucial.






</p></a><h2><a name="sec:known_struct_partial_obs"></a><a name="sec:unknown_struct_full_obs">Unknown structure, full observability</a></h2><a name="sec:unknown_struct_full_obs">

We start by discussing the scoring function which we use to select
models; we then discuss algorithms which attempt to optimize this
function over the space of models, and finally examine their computational and
sample complexity.
<p>

</p></a><h3><a name="sec:unknown_struct_full_obs"></a><a name="sec:cost_fn">The objective function used for model selection</a></h3><a name="sec:cost_fn">


The maximum likelihood model will be a
complete graph, since this has the largest 
number of parameters, and hence can fit the data the best.
A well-principled way to avoid this kind of over-fitting is to put a prior on models,
specifying that we prefer sparse models.
Then, by Bayes' rule, the MAP model is the one that maximizes
<center>
<img src="./Graphical Models_files/bayes_rule.gif">
</center>
<!--\Pr(G|D) = \frac{\Pr(D|G) \Pr(G)} {\Pr(D)}-->
Taking logs, we find
<center>
<img src="./Graphical Models_files/log_bayes_rule.gif">
</center>
<!--\log \Pr(G|D) = \log \Pr(D|G) + \log \Pr(G) + c -->
where c = - \log \Pr(D) is a constant independent of G.
<p>
The effect of the structure prior P(G) is equivalent to penalizing overly complex
models.
However, this is not strictly necessary, since the marginal likelihood
term
</p><pre>P(D|G) = \int_{\theta} P(D|G, \theta)
</pre>
has a similar effect of penalizing models with too many parameters
(this is known as Occam's razor).



</a><h3><a name="sec:cost_fn"></a><a name="sec:algo">Search algorithms for finding the best model</a></h3><a name="sec:algo">

The goal of structure learning is to learn a dag (directed acyclic
graph) that best explains the data. This is an NP-hard problem, since
the number of dag's on N variables is super-exponential in N. (There is no closed
form formula for this, but to give you an idea,
there are 543 dags on 4 nodes, and O(10^18) dags on 10 nodes.)
<p>
If we know the ordering of the nodes, life becomes much simpler,
since we can learn the parent set for
each node independently
(since the score is decomposable), and we don't need to worry about acyclicity constraints.
For each node, there at most
</p><pre>\sum_{k=0}^n \choice{n}{k} = 2^n
</pre>
<!--<img src="Eqns/choice.gif">-->
sets of possible parents for each node, which can be
arranged in a lattice as shown below for n=4.
The problem is to find the highest scoring point in this lattice.
<p>

</p><center>
<img src="./Graphical Models_files/subsets_lattice.gif">
</center>
<p>

There are three obvious ways to search this graph: bottom up, top
down, or middle out.
In the bottom up approach, we 
start at the bottom of the
lattice, and evaluate the score at all points in each successive
level.
We must decide whether the gains in score produced by a
larger parent set is ``worth it''.
The standard approach in the reconstructibility analysis (RA) community uses the fact
that \chi^2(X,Y) \approx I(X,Y) N \ln(4), where N is the number of
samples and I(X,Y) is the mutual information (MI) between X and Y.
Hence we can use a \chi^2 test to decide
whether an increase in the MI score is statistically significant.
(This also gives us some kind of confidence measure on the connections
that we learn.)
Alternatively, we can use a BIC score.
</p><p>

Of course, if we do not know if we have achieved the maximum possible
score, we do not know when to stop searching, and hence we must
evaluate all points in the lattice (although we can obviously use
branch-and-bound). For large n, this is
computationally infeasible, so a common approach is to only search up
until level K (i.e., assume a bound on the maximum number of parents
of each node), which takes O(n ^ K) time.
</p><p>

The obvious way to avoid the exponential cost (and the need for a
bound, K) is to use heuristics to
avoid examining all possible subsets.
(In fact, we must use heuristics of some kind, since the problem of
learning optimal structure is NP-hard \cite{Chickering95}.)
One approach in the RA framework, called Extended Dependency Analysis (EDA)
\cite{Conant88}, is as follows.
Start by evaluating all subsets of size up to two, keep all the ones with
significant (in the \chi^2 sense) MI with the target node, and take the union of the resulting set
as the set of parents.
</p><p>

The disadvantage of this greedy technique is that it will fail to find a set
of parents unless some subset of size two has significant MI with the
target variable. However, a Monte Carlo
simulation in \cite{Conant88} shows that most random relations have
this property.
In addition, highly interdependent sets of parents (which might
fail the pairwise MI test) violate the causal
independence assumption, which is necessary to justify the use of
noisy-OR and similar CPDs.
</p><p>

An alternative technique, popular in the UAI community, is to start
with an initial guess of the model structure (i.e., at a specific
point in the lattice), and then perform local
search, i.e., evaluate the score of neighboring points in the lattice,
and move to the best such point, until we reach a local optimum. We
can use multiple restarts to try to find the global optimum, and to
learn an ensemble of models.
Note that, in the partially observable case, we need to have an
initial guess of the model structure in order to estimate the values
of the hidden nodes, and hence the (expected) score of each model; starting with the fully
disconnected model (i.e., at the bottom of the lattice) would be a bad
idea, since it would lead to a poor estimate.
</p><p>



</p></a><h2><a name="sec:algo"></a><a name="sec:unknown_struct_partial_obs">Unknown structure, partial observability</a></h2><a name="sec:unknown_struct_partial_obs">


Finally, we come to the hardest case of all, where the structure is
unknown and there are hidden variables and/or missing data.
In this case, to compute the Bayesian score, we must marginalize out
the hidden nodes as well as the parameters.
Since this is usually intractable, it is common to usean asymptotic
approximation to the posterior called BIC (Bayesian Information
Criterion), which is defined as follows:
<pre>\log \Pr(D|G) \approx \log \Pr(D|G, \hat{\Theta}_G) -  \frac{\log N}{2} \#G
</pre>
where N is the number of samples,
\hat{\Theta}_G is the ML estimate of the parameters,
and
#G is the dimension of the model.
(In the fully observable case, the dimension of a model is the number
of free parameters. In a model with hidden variables, it might be less
than this.)
The first term is just the likelihood and
the second term is a penalty for model complexity.
(The BIC score is identical to the Minimum Description Length (MDL)
score.)
<p>
Although the BIC score decomposes into a sum of local terms, one per
node,
local search is still expensive, because we need to run EM at each
step to compute \hat{\Theta}. An alternative approach is to do the
local search steps inside of the M step of EM - this is called
Structureal EM, and provably converges to a local maximum of the BIC
score (Friedman, 1997).


</p><h3>Inventing new hidden nodes</h3>


So far, structure learning has meant finding the right connectivity
between pre-existing nodes. A more interesting problem is inventing
hidden nodes on demand. Hidden nodes can make a model much more
compact, as we see below.
<center>
<img src="./Graphical Models_files/hidden.gif">
</center>
(a) A BN with a hidden variable H. (b) The simplest network
that can capture the same distribution without using a hidden variable
(created using arc reversal and node elimination).
If H is binary and the other nodes are trinary, and we assume full
CPTs, the first network has 45 independent parameters, and the second
has 708.
<p>

The standard approach is to keep adding
hidden nodes one at a time, to some part of the network (see below),
performing structure learning at each
step, until the score drops.
One problem is choosing the cardinality (number of possible values)
for the hidden node, and its type of CPD.
Another problem is
choosing where to add the new hidden node.
There is no point making it a child, since hidden children can always
be marginalized away, so we need to find an existing node which needs
a new parent, when the current set of possible parents is not adequate.
</p><p>

\cite{Ramachandran98} use the following heuristic for finding nodes
which need new parents: they consider
a noisy-OR node which is nearly always on, even if its non-leak
parents are off, as an
indicator that there is a missing parent.
Generalizing this technique beyond noisy-ORs is an interesting open
problem.
One approach might be to examine
H(X|Pa(X)):
if this is very high, it means the current set of parents are
inadequate to ``explain'' the residual entropy; if Pa(X) is the
best (in the BIC or \chi^2 sense) set of parents we have been able
to find in the current model, it
suggests we need to create a new node and add it to Pa(X).
</p><p>

A simple heuristic for inventing hidden nodes in the case of DBNs is
to check if the Markov property is being violated for any particular
node. If so, it suggests that we need connections to slices further
back in time. Equivalently, we can add new lag variables
and connect to them.

<!--
<p>
Another heuristic for DBNs might be to first perform a clustering of the
timeseries of the observed variables (see e.g., \cite{Eisen98}), and
then to associate hidden nodes with each cluster.
The result would be a Markov model with a tree-structured
hidden ``backbone'' c.f., \cite{Jordan96}.
This is one possible approach to the problem of learning
hierarchically structured DBNs. (Building hierarchical (object
oriented) BNs and DBNs {\em by hand} is straightforward, and there are
algorithms which can exploit this modular structure to a certain
extent to speed up inference \cite{Koller97,Friedman98_doobn}.)
-->

</p><p>

Of course,
interpreting the ``meaning'' of hidden nodes is always
tricky, especially since they are often unidentifiable,
e.g., we can often switch the interpretation of the true and false states
(assuming for simplicity that the hidden node is binary) provided we
also permute the parameters appropriately. (Symmetries such as this are
one cause of the multiple maxima in the likelihood surface.)
<!--
Our opinion is that fully automated structure discovery techniques can
be useful as hypothesis generators, which can then be tested by
experiment.
-->



</p><h2>Further reading on learning</h2>

The following are good tutorial articles.

</a><ul><a name="sec:unknown_struct_partial_obs">
</a><li><a name="sec:unknown_struct_partial_obs"> W. L. Buntine, 1994.
</a><a href="http://citeseer.nj.nec.com/6938.html">
"Operations for Learning with Graphical Models"</a>,
J. AI Research, 159--225.

</li><li> D. Heckerman, 1996.
<a href="ftp://ftp.research.microsoft.com/pub/tr/TR-95-06.PS">
"A tutorial on learning with Bayesian networks"</a>,
Microsoft Research tech. report, MSR-TR-95-06.

<!--
<li> <a href="hybrid.html#learning">How BNT uses EM</a>.

<li> <a href="learn.html">Learning DBNs</a>.
-->

</li></ul>




<h1><a name="decision">Decision Theory</a></h1><a name="decision">

It is often said that "Decision Theory = Probability Theory  + Utility
Theory".
We have outlined above how we can model joint probability distributions in a
compact way by using sparse graphs to reflect conditional independence
relationships.
It is also possible to decompose multi-attribute utility functions in
a similar way:
we create a node for each term in the sum, which
has as parents all the attributes (random
variables) on which it depends; typically, the utility node(s) will
have action node(s) as parents, since the utility depends both on the
state of the world and the action we perform.
The resulting graph is called an influence diagram.
In principle, we can then use the influence diagram to compute
the optimal (sequence of) action(s) to perform so as to maximimize
expected utility, although this is computationally intractible for all
but the smallest problems.

</a><p><a name="decision">
Classical control theory is mostly concerned with the special case
where the graphical model is a
</a><a href="https://www.cs.ubc.ca/~murphyk/Bayes/kalman.html#what">Linear Dynamical System</a>
and the utility function is negative quadratic loss, e.g., consider a
missile tracking an airplane: its goal is to minimize the squared
distance between itself and the target. When the utility function
and/or the system model becomes more complicated, traditional methods
break down, and one has to use <a href="https://www.cs.ubc.ca/~murphyk/pomdp.html">reinforcement learning</a> to find the
optimal policy (mapping from states to actions).


</p><h1><a name="appl">Applications</a></h1><a name="appl">

The most widely used Bayes Nets are undoubtedly the ones embedded in
Microsoft's products, including the Answer
Wizard of Office 95, the Office Assistant (the bouncy paperclip guy) of
Office 97, and over 30 Technical Support Troubleshooters.

<p>

BNs originally arose out of an attempt to add probabilities to
expert systems, and this is still the most common use for BNs.
A famous example is
QMR-DT, a decision-theoretic reformulation of the Quick Medical
Reference (QMR) model.
</p><p>
</p><center>
<img align="BOTTOM" src="./Graphical Models_files/qmr.gif">
</center>
Here, the top layer represents hidden disease nodes, and the bottom
layer represents observed symptom nodes.
The goal is to infer the posterior probability of each disease given
all the symptoms (which can be present, absent or unknown).

QMR-DT is so densely connected that exact inference is impossible. Various approximation
methods have been used, including sampling, variational and loopy
belief propagation.
</a><p><a name="appl">
Another interesting fielded application is the
</a><a href="http://www.research.microsoft.com/research/dtg/horvitz/vista.htm">
Vista</a> system, developed
by Eric Horvitz.
The Vista system is a decision-theoretic system that has been used at
NASA Mission Control Center in Houston for several years. The system
uses Bayesian networks to interpret live telemetry and provides advice
on the likelihood of alternative failures of the space shuttle's
propulsion systems. It also considers time criticality and recommends
actions of the highest expected utility. The Vista system also employs
decision-theoretic methods for controlling the display of information
to dynamically identify the most important information to highlight.
Horvitz has gone on to attempt to apply similar technology to
Microsoft products, e.g., the Lumiere project.
</p><p>



Special cases of BNs were independently invented by many different
communities, for use in e.g., genetics (linkage analysis), speech
recognition (HMMs), tracking (Kalman fitering), data compression
(density estimation)
and coding (turbocodes), etc.

</p><p>

For examples of other applications, see the
special issue of Proc. ACM 38(3), 1995,
and the <a href="http://www.research.microsoft.com/research/dtg">Microsoft
Decision Theory Group page</a>.

<!--
For examples of other applications, see the
<a href="http://www.research.microsoft.com/research/dtg/acm.html">
special issue on UAI
</a> of Proc. ACM 38(3), 1995.
-->



</p><h2>Applications to biology</h2>

This is one of the hottest areas.
For a review, see
<ul>
<li>
<a href="http://www.sciencemag.org/cgi/content/full/303/5659/799">
Inferring cellular networks using probabilistic graphical models</a>
<it>Science</it>, Nir Friedman, v303 p799, 6 Feb 2004.
</li></ul>


<h1><a name="reading">Recommended introductory reading</a></h1><a name="reading">

<h2>Books</h2>

In reverse chronological order.

</a><ul><a name="reading">

<li> Daphne Koller and Nir Friedman,
"Probabilistic graphical models: principles and techniques",
MIT Press 2009
<p>

</p></li><li> Adnan Darwiche,
"Modeling and reasoning with Bayesian networks",
Cambridge 2009
<p>

</p></li><li>F. V. Jensen.
"Bayesian Networks and Decision Graphs".
Springer.
2001.
<br>
Probably the best introductory book available.

<p>
</p></li><li>
D. Edwards.
"Introduction to Graphical Modelling",  2nd ed.
Springer-Verlag.
2000.
<br>
Good treatment of <em>undirected</em> graphical models from a statistical perspective.


<p>
</p></li><li> J. Pearl.
"Causality".
Cambridge.
2000.
<br>
The definitive book on using causal DAG modeling.



<p>
</p></li><li> R. G. Cowell, A. P. Dawid, S. L. Lauritzen and
D. J. Spiegelhalter.
"Probabilistic Networks and Expert Systems".
Springer-Verlag.
1999.
<br>
Probably the best book available,
although the treatment is restricted to 
 exact inference.

<p>
</p></li><li> M. I. Jordan (ed).
"Learning in Graphical Models".
MIT Press.
1998.
<br>
Loose collection of papers on machine learning, many related to
graphical models.
One of the few books to discuss <em>approximate</em> inference.

<p>
</p></li><li> B. Frey.
"Graphical models for machine learning and digital communication",
MIT Press.
1998.
<br>
Discusses pattern recognition and turbocodes using (directed)
graphical models.

<p>
</p></li></a><li><a name="reading"> E. Castillo and J. M. Gutierrez and A. S. Hadi.
"Expert systems and probabilistic network models".
Springer-Verlag, 1997.
<br>
A </a><a href="http://personales.unican.es/gutierjm/BookCGH.html">Spanish
version</a> is available online for free.

<p>
</p></li><li> F. Jensen.
"An introduction to Bayesian Networks".
UCL Press.
1996.
Out of print.
<br>
Superceded by his 2001 book.

<p>
</p></li><li> S. Lauritzen.
"Graphical Models",
Oxford.
1996.
<br>
The definitive mathematical exposition of the theory of graphical
models.


<p>
</p></li><li> S. Russell and P. Norvig.
"Artificial Intelligence: A Modern Approach".
Prentice Hall.
1995. 
<br>
Popular undergraduate textbook that includes a readable chapter on
directed graphical models.


<p>
</p></li><li> J. Whittaker.
"Graphical Models in Applied Multivariate Statistics",
Wiley.
1990.
<br>
This is the first book published on graphical modelling from a statistics
perspective.


<p>
</p></li><li> R. Neapoliton.
"Probabilistic Reasoning in Expert Systems".
John Wiley &amp; Sons.
1990.

<p>
</p></li><li> J. Pearl.
"Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference."
Morgan Kaufmann.
1988. 
<br>
The book that got it all started!
<!--
This is the first book published on directed graphical models from an
AI/ cognitive science
perspective (rather than a statistics perspective).
-->
A very insightful book, still relevant today.


</li></ul>



<h2>Review articles</h2>

<ul>

<li>
P. Smyth, 1998.
<a href="http://ftp.ics.uci.edu/pub/smyth/papers/prl.ps.Z">
"Belief networks, hidden Markov models, and Markov
random fields: a unifying view"</a>,
Pattern Recognition Letters.

</li><li> E. Charniak, 1991.
<a href="https://www.cs.ubc.ca/~murphyk/Bayes/Charniak_91.pdf">"Bayesian Networks without Tears"</a>, AI magazine.

</li><li>
Sam Roweis &amp; Zoubin Ghahramani, 1999.
<a href="http://www.gatsby.ucl.ac.uk/~roweis/papers/NC110201.pdf">
A Unifying Review of Linear Gaussian Models</a>,
Neural Computation 11(2) (1999) pp.305-345

</li></ul>


<h2> Exact Inference </h2>
<ul>

<li> C. Huang and A. Darwiche, 1996.
<a href="http://www.aub.edu.lb/people/darwiche/Papers/ijar95.pdf">
"Inference in Belief Networks: A procedural guide"</a>,
Intl. J. Approximate Reasoning, 15(3):225-263.

</li><li> R. McEliece and S. M. Aji, 2000.
<!--<a href="http://www.systems.caltech.edu/EE/Faculty/rjm/papers/GDL.ps">-->
<a href="https://www.cs.ubc.ca/~murphyk/Bayes/GDL.pdf">
The Generalized Distributive Law</a>,
IEEE Trans. Inform. Theory, vol. 46, no. 2 (March 2000),
pp. 325--343. 

</li><li> F. Kschischang, B. Frey and H. Loeliger, 2001.
<a href="http://www.cs.toronto.edu/~frey/papers/fgspa.abs.html">Factor
graphs and the sum product algorithm</a>,
IEEE Transactions on Information Theory, February, 2001.

</li><li> M. Peot and R. Shachter, 1991.
"Fusion and propogation with multiple observations in belief networks",
Artificial Intelligence, 48:299-318.

</li></ul>


<h2> Approximate Inference</h2>
<ul>
<li>
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, 1997.
<a href="http://www.cs.berkeley.edu/~jordan/papers/variational-intro.ps.Z">
"An introduction to variational methods for graphical models."</a>

</li><li> D. MacKay, 1998.
<a href="http://www.cs.toronto.edu/~mackay/erice.ps.gz">
"An introduction to Monte Carlo methods"</a>.

</li><li> <a name="Jaakkola98">
T. Jaakkola and M. Jordan, 1998.
</a><a href="http://www.cs.berkeley.edu/~jordan/papers/varqmr.ps.Z">
"Variational probabilistic inference and the QMR-DT database"
</a>
</li></ul>


<h2> Learning </h2>
<ul>
<li> W. L. Buntine, 1994.
<a href="http://www.ultimode.com/~wray/lwgmJAIR.ps.Z">
"Operations for Learning with Graphical Models"</a>,
J. AI Research, 159--225.

</li><li> D. Heckerman, 1996.
<a href="ftp://ftp.research.microsoft.com/pub/tr/TR-95-06.PS">
"A tutorial on learning with Bayesian networks"</a>,
Microsoft Research tech. report, MSR-TR-95-06.

<!--
<li> P. Krause, 1998.
<A HREF="http://www.auai.org/auai-tutes.html/bayesUS_krause.ps.gz">
"Learning probabilistic networks",</a>
Philips Research Labs tech. report.


<li> N. Friedman, 1998.
<a href="http://www.cs.huji.ac.il/~nir/Abstracts/Fr2.html">
"The Bayesian Structural EM Algorithm"</a>,
UAI.
-->
</li></ul>

<h2> DBNs </h2>
<ul>
<li> L. R. Rabiner, 1989.
<a href="https://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf">"A Tutorial in Hidden Markov Models and Selected Applications in Speech Recognition"</a>,
Proc. of the IEEE, 77(2):257--286.

</li><li> Z. Ghahramani, 1998.
<a href="ftp://ftp.cs.toronto.edu/pub/zoubin/vietri.ps.gz"> 
            Learning Dynamic Bayesian Networks </a> 
          In  C.L. Giles and M. Gori (eds.), <em> Adaptive Processing
            of Sequences and Data Structures </em>. Lecture Notes in Artificial
          Intelligence, 168-197. Berlin: Springer-Verlag.
</li></ul>



<!--

<li> Online tutorials
<ul>

<li> <a href="jordan.tutorial.slides.ps">Michael Jordan's tutorial
slides</a> from 1997. Covers directed and undirected models, exact and
variational inference.

<li> <a
href="http://www.gatsby.ucl.ac.uk/~zoubin/NIPStutorial.html">NIPS '99
tutorial slides</a> on unsupervised learning, by Zoubin Ghahramani
and Sam Roweis.

<li> <a href="ames_slides.ps.gz">My tutorial slides</a> on Bayes nets and
BNT. From a presentation given to NASA Ames on 6/16/00.

<li> <a href="http://www.cs.ubc.ca/labs/lci/CIspace/bayes.html">
Bayes applet</a>, illustrates variable elimination.

<li> <a
href="http://www.cs.cmu.edu/~javabayes/Home/node9.html">Fabio Cozman's list </a>.
</ul>
-->


 


</body></html>